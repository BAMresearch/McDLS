# -*- coding: utf-8 -*-
# McFit.py
# Find the reST syntax at http://sphinx-doc.org/rest.html

r'''
A class and supplementary functions for Monte-Carlo fitting of SAXS patterns.
                            *fitting procedures*

It is released under a `Creative Commons CC-BY-SA license
<http://creativecommons.org/licenses/by-sa/3.0/>`_.
Please cite as::

    Brian R. Pauw et al., J. Appl. Cryst. 46, (2013), pp. 365--371
        doi: http://dx.doi.org/10.1107/S0021889813001295

Classes and Functions Defined in This File
==========================================

 - **McSAS**: class containing all the functions required to perform a
   Monte Carlo analysis on small-angle scattering data.
 - **csqr**: least-squares error to use with scipy.optimize.leastsq
 - **Iopt**: Optimize the scaling factor and background level of modeled
   data vs. intensity
 - **csqr_v1**: least-squares for data with known error, size of
   parameter-space not taken into account
 - **Iopt_v1**: old intensity scaling factor optimisation, more robust but
   slower than Iopt
    
Made possible with help from (amongst others)
=============================================

 - | Samuel Tardif
   | Derivations (mostly observability) and checking of mathematics
 - | Jan Skov Pedersen
   | checking of mathematics
 - | Pawel Kwasniewski <kwasniew@esrf.fr>
   | Code cleanup and documentation
 - | Ingo Bressler <ingo.bressler@bam.de>
   | Code cleanup, modification and documentation

A Note on Units
===============

Internally, all length units are in meters, all angle units in degrees
clockwise from top. *Intensity* is in
:math:`\left[ 1 \over {m \cdot sr} \right]`,
*q* in :math:`\left[ 1 \over m \right]`.
The electron density contrast squared,
*drhosqr* is in :math:`\left[ m^{-4} \right]`.
Other units may be used, but if absolute units are supplied and absolute
volume fractions required, meters are necessitated.

Example Usage
=============


*For detailed usage, please see the* :doc:`quickstart`

Fitting a single dataset using all automatic and default parameters
(may go wrong on poorly conditioned input, needs sensibly-spaced datapoints
and good uncertainty estimates).
The dataset is considered to consist of three variables *Q*, *I* and *IERR*::

 McSAS(Q = Q, I = I, IERR = IERR, Plot = True)

Optional parameters can be supplied in parameter-value pairs to finetune
optimisation behaviour::

 A = McSAS(Q = Q, I = I, IERR = numpy.maximum(0.01 * I, E),
           Ncontrib = 200, Convcrit = 1,
           Bounds = array([0.5e-9, 35e-9]),
           Maxiter = 1e5, Histscale = 'log',
           drhosqr = 1e30, Nreps = 100, Plot = True)

'''

import scipy # For many important functions
from scipy import optimize # For the leastsq optimization function
import numpy # For arrays
from numpy import (inf, array, isfinite, reshape, prod, shape, pi, diff, zeros,
                  arange, size, sin, cos, sum, sqrt, linspace, logspace, log10,
                  isnan, ndim)
import os # Miscellaneous operating system interfaces
import time # Timekeeping and timing of objects
import sys # For printing of slightly more advanced messages to stdout
import pickle #for pickle_read and pickle_write

class McSAS(object):
    r"""
    Main class containing all functions required to do Monte Carlo fitting.

    **Required input parameters:**
        - *McData*: data structure generated by the McData class

    **Returns:**

    A McResult object, the most important result of which is the 'Rrep' result. 
    For intermediate storage, store 'Rrep', the other results can be recalculated
    from the McData structure and 'Rrep'.

    McResult contains the following:

        *Imean*: 1D array (*VariableNumber = 0*)
            The fitted intensity, given as the mean of all Nreps results.
        *q*: 1D array (*VariableNumber = 0*)
            Corresponding q values
            (may be different than the input q if *qlims* was used).
        *Istd*: array (*VariableNumber = 0*)
            Standard deviation of the fitted I(q), calculated as the standard 
            deviation of all Nreps results.
        *Rrep*: size array (Nsph x Nreps) (*VariableNumber = 0*)
            Collection of Nsph sphere radii fitted to best represent the
            provided I(q) data. Contains the results of each of *Nreps*
            iterations. This can be used for rebinning without having to
            re-optimize.
        *Screps*: size array (2 x Nreps) (*VariableNumber = 0*)
            Scaling and background values for each repetition.
            Used to display background level in data and fit plot.
    """        

    dataset=None #where Q, PSI, I and IERR is stored, original dataset
    fitdata=None #may be populated with a subset of the aforementioned dataset, limited to q-limits or psi limits and to positive I values alone
    parameters=None #where the fitting and binning settings are stored
    result=None #where all the analysis results are stored, I do not think this needs separation after all into results of analysis and results of interpretation. However, this is a list of dicts, one per variable (as the method, especially for 2D analysis, can deal with more than one random values. analysis results are stored along with the histogrammed results of the first variable with index [0]:
    functions=None #where the used functions are defined, this is where shape changes, smearing, and various forms of integrations are placed.
    calcdata=None #here values and matrices are stored used by the calculations.

    def __init__(self,McData):
        """
        intialization function, takes keyword-value input parameters. 
        input arguments can be one of the aforementioned parameter keyword-value pairs.
        This does the following::
            1. Populates internal data from supplied McData information 
               (This should be replaced later on with a better method)
            2. Runs the Analyse() function which applies the MC fit multiple times
            3. Returns a McResult dictionary

        """
        #initialize
        self.McData=McData #storing the object for internal use if I can do this.
        self.dataset=McData.getdata(dataset='original') #where Q, PSI, I and IERR is stored, original dataset
        self.fitdata=McData.getdata() #may be populated with a subset of the aforementioned dataset, limited to q-limits or psi limits and to positive I values alone
        self.parameters=McData.getparameters() #where the fitting and binning settings are stored
        self.result=list() #where all the analysis results are stored, I do not think this needs separation after all into results of analysis and results of interpretation. However, this is a list of dicts, one per variable (as the method, especially for 2D analysis, can deal with more than one random values. analysis results are stored along with the histogrammed results of the first variable with index [0]:
        self.result.append(dict())
        self.functions=McData.getfunctions() #where the used functions are defined, this is where shape changes, smearing, and various forms of integrations are placed.

        #Analyse
        self.Analyse()

        return self.getresult()

    def Analyse(self):
        """
        This is one of the main MC functions. This function runs the Monte Carlo optimisation a multitude (Nreps) of times
        If convergence is not achieved, it will try again for a maximum of Maxntry attempts.
        """
        #get data
        q=self.getdata('Q')
        I=self.getdata('I')
        E=self.getdata('IERR')
        #get settings
        Priors=self.getpar('Priors')
        Prior=self.getpar('Prior')
        Ncontrib=self.getpar('Ncontrib')
        Nreps=self.getpar('Nreps')
        Convcrit=self.getpar('Convcrit')
        Maxntry=self.getpar('Maxntry')
        #find out how many values a shape is defined by:
        testR=self.functions['RAND']()
        NRval=prod(shape((testR)))

        Rrep = zeros([Ncontrib,NRval,Nreps]) 
        #DEBUG:
        #print 'Rrep: {}'.format(shape(Rrep))
        Niters = zeros([Nreps])
        Irep = zeros([1,prod(shape(I)),Nreps])
        bignow = time.time() #for time estimation and reporting

        #This is the loop that repeats the MC optimization Nreps times, after which we can calculate an uncertainty on the results.
        priorsflag=False
        for nr in arange(0,Nreps):
            if ((Prior==[])and(Priors!=[]))or(priorsflag==True):
                priorsflag=True #this flag needs to be set as prior will be set after the first pass
                self.setpar(Prior=Priors[:,:,nr%size(Priors,2)])
            nt = 0 #keep track of how many failed attempts there have been 
            # do that MC thing! 
            ConVal=inf
            while ConVal>Convcrit:
                #retry in the case we were unlucky in reaching convergence within Maxiter.
                nt+=1
                Rrep[:,:,nr],Irep[:,:,nr],ConVal,Details = self.MCFit(OutputI=True,OutputDetails=True)
                if nt>Maxntry:
                    #this is not a coincidence. We have now tried Maxntry+2 times
                    print "could not reach optimization criterion within {0} attempts, exiting...".format(Maxntry+2)
                    return
            Niters[nr] = Details['Niter'] #keep track of how many iterations were needed to reach convergence

            biglap = time.time() #time management
            # in minutes:
            tottime = (biglap-bignow)/60. #total elapsed time
            avetime = (tottime/(nr+1)) #average time per MC optimization
            remtime = (avetime*Nreps-tottime) #estimated remaining time
            print "\t*finished optimization number {0} of {1} \r\n\t*total elapsed time: {2} minutes \r\n\t*average time per optimization {3} minutes \r\n\t*total time remaining {4} minutes".format(nr+1,Nreps,tottime,avetime,remtime)
        
        #at this point, all MC optimizations have been completed and we can process all Nreps results.
        Imean = numpy.mean(Irep,axis=2) #mean fitted intensity
        Istd = numpy.std(Irep,axis=2) #standard deviation on the fitted intensity, usually not plotted for clarity
        # store in output dict
        self.setresult(**{
            'Rrep':Rrep,
            'Imean':Imean,
            'Istd':Istd,
            'Qfit':q,# can be obtained from self.fitdata 
            'Niter':numpy.mean(Niters)}) #average number of iterations for all repetitions

    ###########################################################################################
    ################################### Monte-carlo procedure #################################
    ###########################################################################################
    def MCFit(self,OutputI=False,OutputDetails=False,OutputIterations=False):
        '''
        Object-oriented, shape-flexible core of the Monte Carlo procedure. Takes optional arguments
        'OutputI' : Returns the fitted intensity besides the result 
        'OutputDetails' : Details of the fitting procedure, number of iterations and so on
        'OutputIterations' : Returns the result on every successful iteration step, useful
            for visualising the entire Monte Carlo optimisation procedure for presentations.
        '''
        #load dataset
        q=self.getdata('Q')
        I=self.getdata('I')
        E=self.getdata('IERR')
        #load parameters
        Ncontrib=self.getpar('Ncontrib')
        Bounds=self.getpar('Bounds')
        Convcrit=self.getpar('Convcrit')
        Rpfactor=self.getpar('Rpfactor')
        Maxiter=self.getpar('Maxiter')
        MaskNegI=self.getpar('MaskNegI')
        StartFromMin=self.getpar('StartFromMin')
        Memsave=self.getpar('Memsave')
        Prior=self.getpar('Prior')


        #find out how many values a shape is defined by:
        Randfunc=self.functions['RAND']
        FFfunc=self.functions['FF']
        VOLfunc=self.functions['VOL']
        SMEARfunc=self.functions['SMEAR']
        testR=Randfunc()
        NRval=prod(shape(testR))

        Rset=numpy.zeros((Ncontrib,NRval))


        # Intialise variables
        FFset = []
        Vset = []
        Niter = 0
        Conval = inf
        Details = dict()
        Ri = 0 #index of sphere to change. We'll sequentially change spheres, which is perfectly random since they are in random order.
        
        #generate initial set of spheres
        if size(Prior)==0:
            if StartFromMin:
                for Rvi in range(NRval): #minimum bound for each value
                    if numpy.min(Bounds[Rvi:Rvi+2])==0:
                        mb=pi/numpy.max(q)
                    else:
                        mb=numpy.min(Bounds[Rvi:Rvi+2])
                    Rset[:,Rvi]=numpy.ones(Ncontrib)[:]*mb/2.
            else:
                Rset=Randfunc(Ncontrib)
        elif (size(Prior,0)!=0)&(size(Ncontrib)==0):
            Ncontrib=size(Prior,0)
            Rset=Prior
        elif size(Prior,0)==Ncontrib:
            Rset=Prior
        elif size(Prior,0)<Ncontrib:
            print "size of prior is smaller than Ncontrib. duplicating random prior values"
            #while size(Prior)<Nsph:
            Addi=numpy.random.randint(size(Prior,0),size=Ncontrib-size(Prior,0))
            Rset=concatenate((Prior,Prior[Addi,:]))
            print "size now:", size(Rset)
        elif size(Prior,0)>Ncontrib:
            print "Size of prior is larger than Ncontrib. removing random prior values"
            Remi=numpy.random.randint(size(Prior,0),size=Ncontrib) #remaining choices
            Rset=Prior[Remi,:]
            print "size now:", size(Rset,0)
        
        if Memsave==False:
            #calculate their form factors
            FFset=FFfunc(Rset)
            Vset=VOLfunc(Rset,Rpfactor)
            #Vset=(4.0/3*pi)*Rset**(3*Rpfactor)
            #calculate the intensities
            Iset=FFset**2*(Vset+0*FFset)**2 #a set of intensities
            Vst=sum(Vset**2) # total volume squared
            It=sum(Iset,0) # the total intensity - eq. (1)
            It=reshape(It,(1,prod(shape(It)))) # reshaped to match I and q
        else:
            #calculate intensity in memory saving mode:
            #calculate volume for entire set, this does not take much space   
            Vset=VOLfunc(Rset,Rpfactor)

            FFset=FFfunc(Rset[0,:][newaxis,:])
            It=FFset**2*(Vset[0]+0*FFset)**2 #a set of intensities
            for ri in arange(1,Ncontrib):
                #calculate their form factors
                FFset=FFfunc(Rset[ri,:][newaxis,:])
                It=It+FFset**2*(Vset[ri]+0*FFset)**2 #a set of intensities
            Vst=sum(Vset**2) # total volume squared
            It=reshape(It,(1,prod(shape(It)))) # reshaped to match I and q

        # Optimize the intensities and calculate convergence criterium
        #SMEAR function goes here
        It=SMEARfunc(It)
        Sci = numpy.max(I)/numpy.max(It) #initial guess for the scaling factor.
        Bgi = numpy.min(I)
        Sc,Conval1=Iopt_v1(I,It/Vst,E,[Sci,Bgi]) # V1 is more robust w.r.t. a poor initial guess
        Sc,Conval=Iopt(I,It/Vst,E,Sc) # reoptimize with V2, there might be a slight discrepancy in the residual definitions of V1 and V2 which would prevent optimization.
        #print "Initial conval V1",Conval1
        print "Initial Chi-squared value",Conval

        if OutputIterations:
            # Output each iteration, starting with number 0. Iterations will be stored 
            # in Details['Itersph'], Details['IterIfit'], Details['IterConval'], 
            # Details['IterSc'] and Details['IterPriorUnaccepted'] listing the 
            # unaccepted number of moves before the recorded accepted move.
            Details['Itersph']=Rset[:,newaxis] #new iterations will (have to) be appended to this, cannot be zero-padded due to size constraints
            Details['IterIfit']=(It/Vst*Sc[0]+Sc[1])[:,newaxis] #ibid.
            Details['IterConVal']=Conval[newaxis]
            Details['IterSc']=Sc[:,newaxis]
            Details['IterPriorUnaccepted']=numpy.array(0)[newaxis]

        #start the MC procedure
        Now=time.time()
        Nmoves=0 #tracking the number of moves
        Nnotaccepted=0
        while (Conval>Convcrit) &(Niter<Maxiter):
            Rt=Randfunc()
            Ft=FFfunc(Rt)
            Vtt=VOLfunc(Rt,Rpfactor)
            Itt=(Ft**2*Vtt**2)
            # Calculate new total intensity
            if Memsave==False:
                Itest=(It-Iset[Ri,:]+Itt) # we do subtractions and additions, which give us another factor 2 improvement in speed over summation and is much more scalable
            else:
                Fo=FFfunc(Rset[Ri,:][newaxis,:])
                Io=(Fo**2*Vset[Ri]**2)
                Itest=(It-Io+Itt)

            #SMEAR function goes here
            Itest=SMEARfunc(Itest)
            Vstest = (sqrt(Vst)-Vset[Ri])**2+Vtt**2
            # optimize intensity and calculate convergence criterium
            Sct,Convalt = Iopt(I,Itest/Vstest,E,Sc) # using version two here for a >10 times speed improvement
            # test if the radius change is an improvement:
            if Convalt<Conval: # it's better
                if Memsave:
                    Rset[Ri,:],It,Vset[Ri],Vst,Sc,Conval=(Rt,Itest,Vtt,Vstest,Sct,Convalt)
                else:
                    Rset[Ri,:],Iset[Ri,:],It,Vset[Ri],Vst,Sc,Conval=(Rt,Itt,Itest,Vtt,Vstest,Sct,Convalt)
                print "Improvement in iteration number %i, Chi-squared value %f of %f\r" %(Niter,Conval,Convcrit),
                Nmoves+=1
                if OutputIterations:
                    # output each iteration, starting with number 0. 
                    # Iterations will be stored in Details['Itersph'], Details['IterIfit'], 
                    # Details['IterConval'], Details['IterSc'] and 
                    # Details['IterPriorUnaccepted'] listing the unaccepted 
                    # number of moves before the recorded accepted move.
                    Details['Itersph']=concatenate((Details['Itersph'],Rset[:,:,newaxis]),axis=1) #new iterations will (have to) be appended to this, cannot be zero-padded due to size constraints
                    Details['IterIfit']=concatenate((Details['IterIfit'],(Itest/Vstest*Sct[0]+Sct[1])[:,newaxis]),axis=1) #ibid.
                    Details['IterConVal']=concatenate((Details['IterConVal'],numpy.array(Convalt)[newaxis]))
                    Details['IterSc']=concatenate((Details['IterSc'],Sct[:,newaxis]),axis=1)
                    Details['IterPriorUnaccepted']=concatenate((Details['IterPriorUnaccepted'],numpy.array(Nnotaccepted)[newaxis]))
                Nnotaccepted=-1
            # else nothing to do
            Ri+=1 # move to next sphere in list
            Ri=Ri%(Ncontrib) # loop if last sphere
            Nnotaccepted+=1 # number of non-accepted moves, resets to zero after accepted move.
            Niter+=1 # add one to the iteration number           
        if Niter>=Maxiter:
            print "exited due to max. number of iterations (%i) reached" %(Niter)
        else:
            print "Normal exit"
        print "Number of iterations per second",Niter/(time.time()-Now+0.001) #the +0.001 seems necessary to prevent a divide by zero error on some Windows systems.   
        print "Number of valid moves",Nmoves
        print "final Chi-squared value %f" %(Conval)
        Details['Niter']=Niter
        Details['Nmoves']=Nmoves
        Details['elapsed']=(time.time()-Now+0.001)

        #Ifinal=sum(Iset,0)/sum(Vset**2)
        Ifinal=It/sum(Vset**2)
        Ifinal=reshape(Ifinal,(1,prod(shape(Ifinal))))
        #SMEAR function goes here
        Ifinal=SMEARfunc(Ifinal)
        Sc,Conval=Iopt(I,Ifinal,E,Sc)    
        if OutputI:
            if OutputDetails:
                #DEBUG:
                #print 'Rset: {}, I: {}, Conval: {}'.format(shape(Rset),shape((Ifinal*Sc[0]+Sc[1])),shape(Conval))

                return Rset,(Ifinal*Sc[0]+Sc[1]),Conval,Details
            else:
                return Rset,(Ifinal*Sc[0]+Sc[1]),Conval
        else:
            if OutputDetails:
                return Rset,Conval,Details # ifinal cannot be output with variable length intensity outputs (in case of masked negative intensities or q limits)
            else:
                return Rset,Conval # ifinal cannot be output with variable length intensity outputs (in case of masked negative intensities or q limits)

    ######################################## end ###############################################

    def _Iopt(self,I0,I1,startval):
        """
        This function does not do anything yet, but is supposed to become the internal intensity fitting function. External functions ``Iopt`` and ``Iopt_v1`` are used instead.
        """
        pass

    def getpar(self,parname=[]):
        '''gets the value of a parameter, so simple it is probably superfluous'''
        if parname==[]:
            return self.parameters
        else:
            return self.parameters[parname]

    def getdata(self,parname=[],dataset='fit'):
        '''gets the values of a dataset, retrieves from fitdata (clipped) by default. If the original data is wanted, use "dataset='original'" as kwarg. '''
        if (parname==[]):
            if (dataset=='fit'):
                return self.fitdata
            else: 
                return self.dataset
        else:
            if (parname in self.fitdata.keys())and(dataset=='fit'):
                return self.fitdata[parname]
            else:
                return self.dataset[parname]

    def getresult(self,parname=[],VariableNumber=0):
        if parname==[]:
            return self.result[VariableNumber]
        else:
            return self.result[VariableNumber][parname]

    def setresult(self,**kwargs):
        '''
        Sets the supplied keyword-value pairs to the result. These can be arbitrary. Varnum is the sequence number of the variable for which data is stored. Default is set to 0, which is where the output of the MC routine is put before histogramming. The Histogramming procedure may populate more variables if so needed.
        '''
        if 'VariableNumber' in kwargs.keys():
            varnum=kwargs['VariableNumber']
        else:
            varnum=0

        while len(self.result)<(varnum+1):
            #make sure there is a dictionary in the location we want to save the result
            self.result.append(dict())
        
        rdict=self.result[varnum]

        for kw in kwargs:
            rdict[kw]=kwargs[kw]

    def setdata(self,**kwargs):
        '''
        Sets the supplied data in the proper location. Optional argument "dataset" can be set to "fit" or "original" to define which dataset is set. Default is "original"
        '''
        datasetlist=list(['Q','I','PSI','IERR']) #list of valid things
        if ('dataset' in kwargs):
            dataset=kwargs['dataset'].lower()
        else:
            dataset='original'
        if not ( dataset in ('fit','original')):
            dataset='original'

        if dataset=='original':
            for kw in kwargs:
                if kw in datasetlist:
                    self.dataset[kw]=kwargs[kw]
                else:
                    pass #do not store non-dataset values.
        else: #we want to store to fitdata: a clipped dataset
            for kw in kwargs:
                if kw in datasetlist:
                    self.fitdata[kw]=kwargs[kw]
                else:
                    pass #do not store non-dataset values.

##general functions
def csqr(Sc,I,Ic,E):
    #least-squares error for use with scipy.optimize.leastsq
    cs=(I-Sc[0]*Ic-Sc[1])/E
    #print "Size E",size(E)
    #print "Sc: %f, %f" %(Sc[0],Sc[1])
    return cs

def Iopt(I,Ic,E,Sc,OutputI=False):
    #new version, using leastsq and csqr, speed improvement of over a factor of 10 w.r.t. V1's use in the MC algorithm
    #optimizes the scaling and background factor to match Ic closest to I. returns an array with scaling factors. Input Sc has to be a two-element array wiht the scaling and background   
    #initial guesses. No bounds at the moment are applied, except that the intensity scaling has to be positive.
    Sc,success=scipy.optimize.leastsq(csqr,Sc,args=(I.flatten(),Ic.flatten(),E.flatten()),full_output=0)
    #print "Sc: %f, %f" %(Sc[0],Sc[1])
    cval=csqr_v1(I,Sc[0]*Ic+Sc[1],E)
    if OutputI:
        return Sc,cval,Sc[0]*Ic+Sc[1]
    else:
        return Sc,cval

def csqr_v1(I,Ic,E):
    #least-squares for data with known error, size of parameter-space not taken into account
    cs=sum(((I-Ic)/E)**2)/(size(I))
    return cs
#
def Iopt_v1(I,Ic,E,Sc,OutputI=False,Background=True):
    #old version, using fmin and csqr_v1
    #optimizes the scaling and background factor to match Ic closest to I. returns an array with scaling factors. Input Sc has to be a two-element array wiht the scaling and background   
    #initial guesses. No bounds at the moment are applied, except that the intensity scaling has to be positive.
    #Background can be set to False to just find the scaling factor.
    if Background:
        Sc=scipy.optimize.fmin(lambda Sc: csqr_v1(I,Sc[0]*Ic+Sc[1],E),Sc,full_output=0, disp=0)
        cval=csqr_v1(I,Sc[0]*Ic+Sc[1],E)
    else:
        Sc[1]=0.
        Sc=scipy.optimize.fmin(lambda Sc: csqr_v1(I,Sc[0]*Ic,E),Sc,full_output=0, disp=0)
        Sc[1]=0.
        cval=csqr_v1(I,Sc[0]*Ic,E)
    if OutputI:
        return Sc,cval,Sc[0]*Ic+Sc[1]
    else:
        return Sc,cval


# vim: set ts=4 sts=4 sw=4 tw=0:
